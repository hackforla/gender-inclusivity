const puppeteer = require('puppeteer'); // v 1.1.0
const { URL } = require('url');
const fse = require('fs-extra'); // v 5.0.0
const path = require('path');
const xml = require('xml2js');
const parser = new xml.Parser();
const Spider = require('./Spider');
let z = 1;

module.exports = {
  createSpider: createSpider,
  getSpiders: getSpiders,
  getSpider: getSpider,
  deleteSpider: deleteSpider,
  putSpider: putSpider
};

// Puppeteer worker.
async function start(urlToFetch, siteName) {
  /* 1 */
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  //await page.setRequestInterception(true);
  /* 2 */
  page.on('response', async (response) => {

    try {
      const status = response.status;

      page.on(  "console", function (log) {
        //   This is for debugging puppeteer
         // console.log(log.text());
      });
      const url = new URL(response.url());

      // This intercept is a method of blocking potential URL redirects
      // console.log(interceptedRequest.url(), url, status);
      // if (interceptedRequest.url().endsWith('/api')) {
      //     interceptedRequest.respond({
      //         status: 422,
      //         body: "FAKE"
      //     })
      // } else interceptedRequest.continue();

      let filePath = path.resolve(`./output/` + siteName + '/' + `${url.pathname}`);
      if (path.extname(url.pathname).trim() === '') {
        //console.log(url.pathname, `${filePath}/index.html`)
        filePath = `${filePath}/index.html`;
      }

      // Save Data
      if ((status >= 300) && (status <= 399)) {
        //console.log('Redirect from', response.url(), 'to', response.headers()['location']);
        await new Promise(function(resolve, reject) {
          resolve(null);
        });
      } else {
        await fse.outputFile(filePath, await response.buffer());
        await fse.outputFile(filePath + z, await response.buffer());
        z++;
      }

    } catch (err) {
      console.log('Redirect from', response.url(), 'to', response.headers()['location'], err, response.status);
    }
  });

  /* 3 */
  await page.goto(urlToFetch, {
    waitUntil: 'load'
  });

  /* 4 */
  setTimeout(async () => {
    await browser.close();
  }, 60000 * 4);
};

// Headless chrome requires a lot or resources.
function createQueue(tasks, maxNumOfWorkers = 4) {
  var numOfWorkers = 0;
  var taskIndex = 0;

  return new Promise( done => {
    const handleResult = index => result => {
      tasks[index] = result;
      numOfWorkers--;
      getNextTask();
    };
    const getNextTask = () => {
      if (numOfWorkers < maxNumOfWorkers && taskIndex < tasks.length) {
        tasks[taskIndex]().then(handleResult(taskIndex)).catch(handleResult(taskIndex));
        taskIndex++;
        numOfWorkers++;
        getNextTask();
      } else if (numOfWorkers === 0 && taskIndex === tasks.length) {
        done(tasks);
      }
    };
    getNextTask();
  });
};

// Read sites generated by web indexer.
let xmlInjest = (siteName, path) => {
  let items = [];
  return new Promise(function(resolve, reject) {
    fse.readFile(__dirname + path, (err, data) => {
      parser.parseString(data, (err, result) => {
        result.urlset.url.forEach((item, index) => {
          items.push(async () => {
            //console.log(item.loc[0]);
            return start(item.loc[0], siteName);
          });
        });
        // Read X items, Y at a time.
        createQueue(items.slice(0,2), 1).then((data) => {
          resolve({
            'siteName': siteName,
            'path': path
          });
        });
      });
    });
  });
};



// Base HTTP REST stubs. We could go for graphQL lets keep REST for now.
function createSpider(req, res, next) {
  return Spider.create({
    title : req.body.title,
    description : req.body.description,
    pinned : req.body.pinned
  })
  .then(note => res.status(200).send(note))
  .catch(err => next(new Error(err)));
}

function getSpiders(req, res, next) {
  return Spider.find({})
  .then(Spiders => res.status(200).send(notes))
  .catch(err => next(new Error(err)));
}

function getSpider(req, res, next) {
  return Spider.findById(req.params.id)
  .then(Spider => {
    if (!Spider) return res.status(404).send('No Spider found.');
    res.status(200).send(Spider);
  })
  .catch(err => next(new Error(err)));
}

function deleteSpider(req, res, next) {
  return Spider.findByIdAndRemove(req.params.id)
  .then(Spider => res.status(200).send(Spider))
  .catch(err => next(new Error(err)));
}

function putSpider(req, res, next) {
  // in the future we want to use mongo or some other DB to cache requests?
  // return Spider.findByIdAndUpdate(req.params.id, req.body, {new: true})
  //   .then(Spider => res.status(200).send(Spider))
  //   .catch(err => next(new Error(err)));

  // examples xmlInjest('www-lamayor', '/siteMap.xml');
  xmlInjest('www-lamayor', '/siteMap.xml').then((data) => {
    res.send({
      'worker': data || '??????',
      'input': {
        'params': req.params,
        'body': req.body
      }
    });
  });
}

